# Mini-Projet XAI â€” Explication pour la DÃ©tection d'Objets

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ“ Description

Ce projet illustre comment adapter les techniques d'**ExplicabilitÃ© de l'IA (XAI)**, notamment **Grad-CAM**, pour expliquer les prÃ©dictions de modÃ¨les de dÃ©tection d'objets. Contrairement Ã  la classification simple oÃ¹ l'on explique une classe, nous expliquons ici *pourquoi une boÃ®te englobante a Ã©tÃ© prÃ©dite Ã  cet endroit prÃ©cis*.

---

## ğŸ‘¥ Auteurs

- **Zouga Mouhcine**
- **Amllal Amine**

**Date de rÃ©alisation :** 11 dÃ©cembre 2024

---

## ğŸ¯ Objectifs PÃ©dagogiques

Ce mini-projet s'inscrit dans le cadre d'un cours sur l'ExplicabilitÃ© de l'IA (XAI) et vise Ã  :

1. **DÃ©couvrir** une adaptation de Grad-CAM pour la dÃ©tection d'objets
2. **Comprendre conceptuellement** comment expliquer les dÃ©cisions d'un dÃ©tecteur d'objets
3. **ImplÃ©menter** une solution fonctionnelle avec du code production-ready
4. **Analyser** les forces et limites de cette approche XAI

---

## ğŸ” Contexte & Motivation

### Le ProblÃ¨me

Les modÃ¨les de **dÃ©tection d'objets** (YOLO, Faster R-CNN, SSD, etc.) sont aujourd'hui omniprÃ©sents dans :
- ğŸš— VÃ©hicules autonomes
- ğŸ“¹ Surveillance vidÃ©o
- ğŸ¥ Imagerie mÃ©dicale
- ğŸ­ ContrÃ´le qualitÃ© industriel

Ces modÃ¨les prÃ©disent simultanÃ©ment :
- **OÃ¹** se trouvent les objets (boÃ®tes englobantes)
- **Quoi** sont ces objets (classe)
- **Ã€ quel point** le modÃ¨le est confiant

Cependant, ces rÃ©seaux de neurones profonds sont des **boÃ®tes noires** : ils fournissent des prÃ©dictions sans expliquer *pourquoi*.

### Pourquoi l'ExplicabilitÃ© ?

Dans des contextes critiques (mÃ©decine, sÃ©curitÃ©, justice), il est essentiel de pouvoir :
- âœ… **Comprendre** les dÃ©cisions du modÃ¨le
- âœ… **VÃ©rifier** qu'il se base sur les bonnes caractÃ©ristiques visuelles
- âœ… **DÃ©tecter** les biais ou les raccourcis appris

---

## ğŸ§  MÃ©thode XAI UtilisÃ©e : Grad-CAM AdaptÃ©

### Grad-CAM (Gradient-weighted Class Activation Mapping)

**Grad-CAM** est une technique qui utilise les gradients (rÃ©tropropagation) pour comprendre quelles parties de l'image ont le plus influencÃ© la dÃ©cision du modÃ¨le.

### Adaptation pour la DÃ©tection d'Objets

Au lieu de cibler le score d'une classe (classification), nous ciblons :
- Le **score de confiance d'une boÃ®te spÃ©cifique**
- Le **score de classe associÃ© Ã  cette boÃ®te**

**RÃ©sultat :** Des **heatmaps** (cartes de chaleur) montrant les rÃ©gions importantes pour chaque dÃ©tection.

### Famille XAI

- ğŸ·ï¸ **Type** : BasÃ©e sur les gradients
- ğŸ¯ **Explication** : Locale (une boÃ®te spÃ©cifique)
- ğŸ“Š **Sortie** : Heatmap visuelle
- âš¡ **Performance** : Rapide (un seul forward + backward pass)

---

## ğŸ› ï¸ Architecture Technique

### ModÃ¨le de DÃ©tection

- **Architecture** : Faster R-CNN
- **Backbone** : ResNet-50 + FPN (Feature Pyramid Network)
- **Dataset d'entraÃ®nement** : COCO (80 classes d'objets)
- **Source** : ModÃ¨le prÃ©-entraÃ®nÃ© de torchvision

### Pipeline Grad-CAM

```
Image â†’ CNN Backbone â†’ Feature Maps (A^k) â†’ DÃ©tection Head â†’ BoÃ®tes + Scores
                              â†“                                    â†“
                        Gradients (âˆ‚y/âˆ‚A) â†â†â†â†â†â†â†â†â†â†â†â†â†â† Score cible (y^c)
                              â†“
                    Poids Î± = moyenne(gradients)
                              â†“
                    Heatmap = ReLU(Î£ Î±_k Â· A^k)
```

---

## ğŸ“¦ Installation & DÃ©pendances

### PrÃ©requis

- Python 3.8+
- CUDA (optionnel, pour GPU)

### Installation

```bash
# Installation des dÃ©pendances principales
pip install torch torchvision
pip install grad-cam
pip install opencv-python
pip install requests pillow matplotlib numpy
```

### Packages UtilisÃ©s

| Package | Version | Usage |
|---------|---------|-------|
| PyTorch | 2.0+ | ModÃ¨le de dÃ©tection et calcul des gradients |
| Torchvision | 0.15+ | ModÃ¨le Faster R-CNN prÃ©-entraÃ®nÃ© |
| OpenCV | 4.0+ | Traitement d'images |
| Matplotlib | 3.5+ | Visualisations |
| NumPy | 1.20+ | Calculs numÃ©riques |

---

## ğŸš€ Utilisation

### ExÃ©cution du Notebook

1. Ouvrir `Mini_Projet_XAI_Detection_Objets.ipynb`
2. ExÃ©cuter les cellules sÃ©quentiellement
3. Les images de test sont chargÃ©es automatiquement depuis Unsplash

### Sections du Notebook

1. **Installation des dÃ©pendances**
2. **Chargement du modÃ¨le Faster R-CNN**
3. **Chargement des images de test**
4. **DÃ©tection d'objets**
5. **ImplÃ©mentation de Grad-CAM pour la dÃ©tection**
6. **GÃ©nÃ©ration des heatmaps d'explication**
7. **Visualisations multiples** (superpositions, comparaisons)
8. **Analyse dÃ©taillÃ©e** avec mÃ©triques quantitatives
9. **InterprÃ©tation des rÃ©sultats**

---

## ğŸ“Š RÃ©sultats & Visualisations

Le notebook gÃ©nÃ¨re plusieurs types de visualisations :

### 1. Heatmaps Grad-CAM
- Heatmap brute (colormap hot/jet)
- Superposition sur l'image originale
- Comparaison de plusieurs colormaps (JET, Inferno)

### 2. Analyses Quantitatives
- **Score d'alignement** : mesure si la heatmap est bien concentrÃ©e dans la boÃ®te
- **Statistiques** : valeur max, moyenne, pourcentage de pixels activÃ©s
- **Indicateurs de qualitÃ©** : ğŸŸ¢ Excellent / ğŸŸ¡ Bon / ğŸ”´ ProblÃ©matique

### 3. Comparaisons Multi-Images
- Traitement automatique de plusieurs images
- Explications pour les 3 meilleures dÃ©tections par image

---

## ğŸ’¡ Points ClÃ©s Ã  Retenir

### âœ… Forces de Grad-CAM pour la DÃ©tection

| Aspect | Ã‰valuation |
|--------|------------|
| **RapiditÃ©** | â­â­â­â­â­ Un seul forward + backward pass |
| **SimplicitÃ©** | â­â­â­â­ Facile Ã  implÃ©menter et comprendre |
| **InterprÃ©tabilitÃ©** | â­â­â­â­ Heatmaps visuellement intuitives |
| **FlexibilitÃ©** | â­â­â­â­ Applicable Ã  tout CNN avec feature maps |

**Avantages par rapport Ã  d'autres mÃ©thodes :**
- **vs LIME** : Pas besoin de perturbations multiples (plus rapide)
- **vs SHAP** : Pas de calcul combinatoire coÃ»teux
- **vs Saliency Maps** : Plus lisses et moins bruitÃ©es

### âš ï¸ Limites et PiÃ¨ges

1. **RÃ©solution limitÃ©e** : Les feature maps de la derniÃ¨re couche sont de basse rÃ©solution
2. **Isolation imparfaite** : Difficile d'isoler parfaitement une seule boÃ®te
3. **SensibilitÃ© Ã  l'architecture** : Le choix de la couche cible influence les rÃ©sultats
4. **Pas d'incertitude** : La heatmap ne quantifie pas l'incertitude
5. **Risque de biais** : Le modÃ¨le peut utiliser des corrÃ©lations spurieuses

### ğŸ¯ Contextes d'Utilisation

| Contexte | Recommandation |
|----------|----------------|
| **Debugging de modÃ¨le** | âœ… TrÃ¨s utile |
| **Communication aux non-experts** | âœ… Les heatmaps sont intuitives |
| **DÃ©cisions critiques** | âš ï¸ Ã€ utiliser en complÃ©ment |
| **Certification/Audit** | âŒ Insuffisant seul |

---

## ğŸ”¬ Extensions Possibles

- **D-RISE** : MÃ©thode de perturbation spÃ©cifique Ã  la dÃ©tection
- **Grad-CAM++** : Version amÃ©liorÃ©e avec meilleure localisation
- **Score-CAM** : Alternative sans gradients, plus stable
- **Attention Maps** : Pour architectures avec mÃ©canismes d'attention (DETR, ViT)
- **Contrefactuels visuels** : "Que changer pour que la dÃ©tection disparaisse ?"

---

## ğŸ“š RÃ©fÃ©rences

### Articles Scientifiques

1. **Grad-CAM (original)**  
   Selvaraju et al. (2017) - *"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"*  
   ICCV 2017. [arXiv:1610.02391](https://arxiv.org/abs/1610.02391)

2. **D-RISE pour la dÃ©tection**  
   Petsiuk et al. (2021) - *"Black-box Explanation of Object Detectors via Saliency Maps"*  
   CVPR 2021. [arXiv:2006.03204](https://arxiv.org/abs/2006.03204)

3. **Grad-CAM++**  
   Chattopadhay et al. (2018) - *"Grad-CAM++: Generalized Gradient-based Visual Explanations"*  
   WACV 2018.

### Documentation Technique

- [PyTorch Grad-CAM](https://github.com/jacobgil/pytorch-grad-cam)
- [Torchvision Detection Models](https://pytorch.org/vision/stable/models.html#object-detection)
- [Ultralytics YOLO](https://docs.ultralytics.com/)
- [Captum (PyTorch Interpretability)](https://captum.ai/)

---

## ğŸ“‚ Structure du Projet

```
XAI/
â”‚
â”œâ”€â”€ Mini_Projet_XAI_Detection_Objets.ipynb    # Notebook principal
â”œâ”€â”€ README.md                                  # Ce fichier
â””â”€â”€ (images gÃ©nÃ©rÃ©es lors de l'exÃ©cution)
```

---

## ğŸ¤ Contribution

Ce projet est rÃ©alisÃ© dans un cadre pÃ©dagogique. Les contributions sont les bienvenues pour :
- Tester d'autres modÃ¨les de dÃ©tection (YOLO, DETR)
- ImplÃ©menter d'autres mÃ©thodes XAI (D-RISE, Score-CAM)
- AmÃ©liorer les visualisations
- Ajouter des mÃ©triques d'Ã©valuation

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

## ğŸ™ Remerciements

- Ã‰quipe PyTorch pour les modÃ¨les prÃ©-entraÃ®nÃ©s
- Jacob Gildenblat pour la librairie pytorch-grad-cam
- Unsplash pour les images de test gratuites
- Professeurs et encadrants du cours XAI

---

## ğŸ“§ Contact

Pour toute question ou suggestion :
- **Zouga Mouhcine**
- **Amllal Amine**

---

**Note :** Ce projet dÃ©montre que les techniques XAI ne se limitent pas Ã  la classification simple, mais peuvent Ãªtre adaptÃ©es Ã  des tÃ¢ches complexes comme la dÃ©tection d'objets, ouvrant la voie Ã  une IA plus transparente et comprÃ©hensible.
